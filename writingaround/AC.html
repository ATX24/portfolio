<html><head><meta http-equiv="Content-Type" content="text/html; charset=utf-8"/><title>Artificial Consciousness</title><style>
/* cspell:disable-file */
/* webkit printing magic: print all background colors */
html {
	-webkit-print-color-adjust: exact;
}
* {
	box-sizing: border-box;
	-webkit-print-color-adjust: exact;
}

html,
body {
	margin: 0;
	padding: 0;
}
@media only screen {
	body {
		margin: 2em auto;
		max-width: 900px;
		color: rgb(55, 53, 47);
	}
}

body {
	line-height: 1.5;
	white-space: pre-wrap;
}

a,
a.visited {
	color: inherit;
	text-decoration: underline;
}

.pdf-relative-link-path {
	font-size: 80%;
	color: #444;
}

h1,
h2,
h3 {
	letter-spacing: -0.01em;
	line-height: 1.2;
	font-weight: 600;
	margin-bottom: 0;
}

.page-title {
	font-size: 2.5rem;
	font-weight: 700;
	margin-top: 0;
	margin-bottom: 0.75em;
}

h1 {
	font-size: 1.875rem;
	margin-top: 1.875rem;
}

h2 {
	font-size: 1.5rem;
	margin-top: 1.5rem;
}

h3 {
	font-size: 1.25rem;
	margin-top: 1.25rem;
}

.source {
	border: 1px solid #ddd;
	border-radius: 3px;
	padding: 1.5em;
	word-break: break-all;
}

.callout {
	border-radius: 3px;
	padding: 1rem;
}

figure {
	margin: 1.25em 0;
	page-break-inside: avoid;
}

figcaption {
	opacity: 0.5;
	font-size: 85%;
	margin-top: 0.5em;
}

mark {
	background-color: transparent;
}

.indented {
	padding-left: 1.5em;
}

hr {
	background: transparent;
	display: block;
	width: 100%;
	height: 1px;
	visibility: visible;
	border: none;
	border-bottom: 1px solid rgba(55, 53, 47, 0.09);
}

img {
	max-width: 100%;
}

@media only print {
	img {
		max-height: 100vh;
		object-fit: contain;
	}
}

@page {
	margin: 1in;
}

.collection-content {
	font-size: 0.875rem;
}

.column-list {
	display: flex;
	justify-content: space-between;
}

.column {
	padding: 0 1em;
}

.column:first-child {
	padding-left: 0;
}

.column:last-child {
	padding-right: 0;
}

.table_of_contents-item {
	display: block;
	font-size: 0.875rem;
	line-height: 1.3;
	padding: 0.125rem;
}

.table_of_contents-indent-1 {
	margin-left: 1.5rem;
}

.table_of_contents-indent-2 {
	margin-left: 3rem;
}

.table_of_contents-indent-3 {
	margin-left: 4.5rem;
}

.table_of_contents-link {
	text-decoration: none;
	opacity: 0.7;
	border-bottom: 1px solid rgba(55, 53, 47, 0.18);
}

table,
th,
td {
	border: 1px solid rgba(55, 53, 47, 0.09);
	border-collapse: collapse;
}

table {
	border-left: none;
	border-right: none;
}

th,
td {
	font-weight: normal;
	padding: 0.25em 0.5em;
	line-height: 1.5;
	min-height: 1.5em;
	text-align: left;
}

th {
	color: rgba(55, 53, 47, 0.6);
}

ol,
ul {
	margin: 0;
	margin-block-start: 0.6em;
	margin-block-end: 0.6em;
}

li > ol:first-child,
li > ul:first-child {
	margin-block-start: 0.6em;
}

ul > li {
	list-style: disc;
}

ul.to-do-list {
	padding-inline-start: 0;
}

ul.to-do-list > li {
	list-style: none;
}

.to-do-children-checked {
	text-decoration: line-through;
	opacity: 0.375;
}

ul.toggle > li {
	list-style: none;
}

ul {
	padding-inline-start: 1.7em;
}

ul > li {
	padding-left: 0.1em;
}

ol {
	padding-inline-start: 1.6em;
}

ol > li {
	padding-left: 0.2em;
}

.mono ol {
	padding-inline-start: 2em;
}

.mono ol > li {
	text-indent: -0.4em;
}

.toggle {
	padding-inline-start: 0em;
	list-style-type: none;
}

/* Indent toggle children */
.toggle > li > details {
	padding-left: 1.7em;
}

.toggle > li > details > summary {
	margin-left: -1.1em;
}

.selected-value {
	display: inline-block;
	padding: 0 0.5em;
	background: rgba(206, 205, 202, 0.5);
	border-radius: 3px;
	margin-right: 0.5em;
	margin-top: 0.3em;
	margin-bottom: 0.3em;
	white-space: nowrap;
}

.collection-title {
	display: inline-block;
	margin-right: 1em;
}

.page-description {
    margin-bottom: 2em;
}

.simple-table {
	margin-top: 1em;
	font-size: 0.875rem;
	empty-cells: show;
}
.simple-table td {
	height: 29px;
	min-width: 120px;
}

.simple-table th {
	height: 29px;
	min-width: 120px;
}

.simple-table-header-color {
	background: rgb(247, 246, 243);
	color: black;
}
.simple-table-header {
	font-weight: 500;
}

time {
	opacity: 0.5;
}

.icon {
	display: inline-block;
	max-width: 1.2em;
	max-height: 1.2em;
	text-decoration: none;
	vertical-align: text-bottom;
	margin-right: 0.5em;
}

img.icon {
	border-radius: 3px;
}

.user-icon {
	width: 1.5em;
	height: 1.5em;
	border-radius: 100%;
	margin-right: 0.5rem;
}

.user-icon-inner {
	font-size: 0.8em;
}

.text-icon {
	border: 1px solid #000;
	text-align: center;
}

.page-cover-image {
	display: block;
	object-fit: cover;
	width: 100%;
	max-height: 30vh;
}

.page-header-icon {
	font-size: 3rem;
	margin-bottom: 1rem;
}

.page-header-icon-with-cover {
	margin-top: -0.72em;
	margin-left: 0.07em;
}

.page-header-icon img {
	border-radius: 3px;
}

.link-to-page {
	margin: 1em 0;
	padding: 0;
	border: none;
	font-weight: 500;
}

p > .user {
	opacity: 0.5;
}

td > .user,
td > time {
	white-space: nowrap;
}

input[type="checkbox"] {
	transform: scale(1.5);
	margin-right: 0.6em;
	vertical-align: middle;
}

p {
	margin-top: 0.5em;
	margin-bottom: 0.5em;
}

.image {
	border: none;
	margin: 1.5em 0;
	padding: 0;
	border-radius: 0;
	text-align: center;
}

.code,
code {
	background: rgba(135, 131, 120, 0.15);
	border-radius: 3px;
	padding: 0.2em 0.4em;
	border-radius: 3px;
	font-size: 85%;
	tab-size: 2;
}

code {
	color: #eb5757;
}

.code {
	padding: 1.5em 1em;
}

.code-wrap {
	white-space: pre-wrap;
	word-break: break-all;
}

.code > code {
	background: none;
	padding: 0;
	font-size: 100%;
	color: inherit;
}

blockquote {
	font-size: 1.25em;
	margin: 1em 0;
	padding-left: 1em;
	border-left: 3px solid rgb(55, 53, 47);
}

.bookmark {
	text-decoration: none;
	max-height: 8em;
	padding: 0;
	display: flex;
	width: 100%;
	align-items: stretch;
}

.bookmark-title {
	font-size: 0.85em;
	overflow: hidden;
	text-overflow: ellipsis;
	height: 1.75em;
	white-space: nowrap;
}

.bookmark-text {
	display: flex;
	flex-direction: column;
}

.bookmark-info {
	flex: 4 1 180px;
	padding: 12px 14px 14px;
	display: flex;
	flex-direction: column;
	justify-content: space-between;
}

.bookmark-image {
	width: 33%;
	flex: 1 1 180px;
	display: block;
	position: relative;
	object-fit: cover;
	border-radius: 1px;
}

.bookmark-description {
	color: rgba(55, 53, 47, 0.6);
	font-size: 0.75em;
	overflow: hidden;
	max-height: 4.5em;
	word-break: break-word;
}

.bookmark-href {
	font-size: 0.75em;
	margin-top: 0.25em;
}

.sans { font-family: ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol"; }
.code { font-family: "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace; }
.serif { font-family: Lyon-Text, Georgia, ui-serif, serif; }
.mono { font-family: iawriter-mono, Nitti, Menlo, Courier, monospace; }
.pdf .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK JP'; }
.pdf:lang(zh-CN) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK SC'; }
.pdf:lang(zh-TW) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK TC'; }
.pdf:lang(ko-KR) .sans { font-family: Inter, ui-sans-serif, -apple-system, BlinkMacSystemFont, "Segoe UI", Helvetica, "Apple Color Emoji", Arial, sans-serif, "Segoe UI Emoji", "Segoe UI Symbol", 'Twemoji', 'Noto Color Emoji', 'Noto Sans CJK KR'; }
.pdf .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .code { font-family: Source Code Pro, "SFMono-Regular", Menlo, Consolas, "PT Mono", "Liberation Mono", Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.pdf .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK JP'; }
.pdf:lang(zh-CN) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK SC'; }
.pdf:lang(zh-TW) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK TC'; }
.pdf:lang(ko-KR) .serif { font-family: PT Serif, Lyon-Text, Georgia, ui-serif, serif, 'Twemoji', 'Noto Color Emoji', 'Noto Serif CJK KR'; }
.pdf .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK JP'; }
.pdf:lang(zh-CN) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK SC'; }
.pdf:lang(zh-TW) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK TC'; }
.pdf:lang(ko-KR) .mono { font-family: PT Mono, iawriter-mono, Nitti, Menlo, Courier, monospace, 'Twemoji', 'Noto Color Emoji', 'Noto Sans Mono CJK KR'; }
.highlight-default {
	color: rgba(55, 53, 47, 1);
}
.highlight-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.highlight-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.highlight-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.highlight-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.highlight-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.highlight-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.highlight-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.highlight-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.highlight-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.highlight-gray_background {
	background: rgba(241, 241, 239, 1);
}
.highlight-brown_background {
	background: rgba(244, 238, 238, 1);
}
.highlight-orange_background {
	background: rgba(251, 236, 221, 1);
}
.highlight-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.highlight-teal_background {
	background: rgba(237, 243, 236, 1);
}
.highlight-blue_background {
	background: rgba(231, 243, 248, 1);
}
.highlight-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.highlight-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.highlight-red_background {
	background: rgba(253, 235, 236, 1);
}
.block-color-default {
	color: inherit;
	fill: inherit;
}
.block-color-gray {
	color: rgba(120, 119, 116, 1);
	fill: rgba(120, 119, 116, 1);
}
.block-color-brown {
	color: rgba(159, 107, 83, 1);
	fill: rgba(159, 107, 83, 1);
}
.block-color-orange {
	color: rgba(217, 115, 13, 1);
	fill: rgba(217, 115, 13, 1);
}
.block-color-yellow {
	color: rgba(203, 145, 47, 1);
	fill: rgba(203, 145, 47, 1);
}
.block-color-teal {
	color: rgba(68, 131, 97, 1);
	fill: rgba(68, 131, 97, 1);
}
.block-color-blue {
	color: rgba(51, 126, 169, 1);
	fill: rgba(51, 126, 169, 1);
}
.block-color-purple {
	color: rgba(144, 101, 176, 1);
	fill: rgba(144, 101, 176, 1);
}
.block-color-pink {
	color: rgba(193, 76, 138, 1);
	fill: rgba(193, 76, 138, 1);
}
.block-color-red {
	color: rgba(212, 76, 71, 1);
	fill: rgba(212, 76, 71, 1);
}
.block-color-gray_background {
	background: rgba(241, 241, 239, 1);
}
.block-color-brown_background {
	background: rgba(244, 238, 238, 1);
}
.block-color-orange_background {
	background: rgba(251, 236, 221, 1);
}
.block-color-yellow_background {
	background: rgba(251, 243, 219, 1);
}
.block-color-teal_background {
	background: rgba(237, 243, 236, 1);
}
.block-color-blue_background {
	background: rgba(231, 243, 248, 1);
}
.block-color-purple_background {
	background: rgba(244, 240, 247, 0.8);
}
.block-color-pink_background {
	background: rgba(249, 238, 243, 0.8);
}
.block-color-red_background {
	background: rgba(253, 235, 236, 1);
}
.select-value-color-uiBlue { background-color: rgba(35, 131, 226, .07); }
.select-value-color-pink { background-color: rgba(245, 224, 233, 1); }
.select-value-color-purple { background-color: rgba(232, 222, 238, 1); }
.select-value-color-green { background-color: rgba(219, 237, 219, 1); }
.select-value-color-gray { background-color: rgba(227, 226, 224, 1); }
.select-value-color-transparentGray { background-color: rgba(227, 226, 224, 0); }
.select-value-color-translucentGray { background-color: rgba(255, 255, 255, 0.0375); }
.select-value-color-orange { background-color: rgba(250, 222, 201, 1); }
.select-value-color-brown { background-color: rgba(238, 224, 218, 1); }
.select-value-color-red { background-color: rgba(255, 226, 221, 1); }
.select-value-color-yellow { background-color: rgba(253, 236, 200, 1); }
.select-value-color-blue { background-color: rgba(211, 229, 239, 1); }
.select-value-color-pageGlass { background-color: undefined; }
.select-value-color-washGlass { background-color: undefined; }

.checkbox {
	display: inline-flex;
	vertical-align: text-bottom;
	width: 16;
	height: 16;
	background-size: 16px;
	margin-left: 2px;
	margin-right: 5px;
}

.checkbox-on {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20width%3D%2216%22%20height%3D%2216%22%20fill%3D%22%2358A9D7%22%2F%3E%0A%3Cpath%20d%3D%22M6.71429%2012.2852L14%204.9995L12.7143%203.71436L6.71429%209.71378L3.28571%206.2831L2%207.57092L6.71429%2012.2852Z%22%20fill%3D%22white%22%2F%3E%0A%3C%2Fsvg%3E");
}

.checkbox-off {
	background-image: url("data:image/svg+xml;charset=UTF-8,%3Csvg%20width%3D%2216%22%20height%3D%2216%22%20viewBox%3D%220%200%2016%2016%22%20fill%3D%22none%22%20xmlns%3D%22http%3A%2F%2Fwww.w3.org%2F2000%2Fsvg%22%3E%0A%3Crect%20x%3D%220.75%22%20y%3D%220.75%22%20width%3D%2214.5%22%20height%3D%2214.5%22%20fill%3D%22white%22%20stroke%3D%22%2336352F%22%20stroke-width%3D%221.5%22%2F%3E%0A%3C%2Fsvg%3E");
}
	
</style></head><body><article id="a3f37c02-c20c-47d4-bf7b-ed0ec67ddbe8" class="page sans"><header><h1 class="page-title">Artificial Consciousness</h1><p class="page-description"></p></header><div class="page-body"><p id="d7ee4c4c-566d-4cc2-bd33-23517cd15149" class="">5/20/2024 </p><p id="b4bc3c76-4ffd-487a-82c0-795b25659b82" class="">Dhilan Shah </p><h1 id="352e957c-d913-4dd4-80ac-716d278f2983" class=""><strong>Introduction</strong></h1><p id="a9a82aaa-3d3b-4005-9a80-ce6cddd6524e" class="">ChatGPT and other LLMs are passing rigorous Turing tests at a troubling rate. Consequently, AI seems to be the quintessential topic in the tech community. Every tech company worth anything has the words “machine learning” plastered all over their corporate offices. Wall Street is pouring billions into AI startups every hour. Even academia is facing a revolution in AI citation verification. Yet, there seems to be a general misunderstanding surrounding machine learning and artificial intelligence. Additionally, there is a surprising lack of research surrounding new possible tests for AI consciousness. This article intends to fill both gaps by explaining the basic principles behind deep learning and neural networks and also providing a contemporary test for AI consciousness.</p><h1 id="2bb2c7be-48ad-46dc-a923-334b9df1243c" class="">Definition of Consciousness</h1><h2 id="40559452-20be-4d73-952b-4faae780dc24" class=""><em>The “Hard” problem of consciousness</em></h2><p id="ca4ded89-3867-4010-a276-8e5ab0b259d3" class="">Studying consciousness with our modern understanding of the brain and nature is like performing back surgery without any formal training. In other words, most scholars have no idea what the hell they’re doing.</p><p id="065c93da-9b42-4189-84f5-aad206e7a9c8" class="">
</p><p id="048044f0-1c92-4df3-91ed-43e2d09d2947" class="">However, researchers do have enough tools to have a simple illustration of the problem itself - much like how you don’t need surgery to know when you’ve pulled your back. One particular scholar troubled with a variety of back problems is David Chalmers, a philosophy professor at the Austrian National University. In his famous paper, “Facing up to the Problem of Consciousness,” he successfully breaks the problem of consciousness into two sub-sections. The first aspect of consciousness is the “easy problem,” which Chalmers defines as behaviors that “seem directly susceptible to the standard methods of cognitive science, whereby a phenomenon is explained in terms of computational or neural mechanisms” (1995). In other words, the easy problems of consciousness are explainable; sensory experiences give rise to reactions in various neurological structures that some may naively call consciousness. However, these reactions only contribute to half of the equation. Chalmers explains how the “really hard problem of consciousness is the problem of experience.” Right now, we can’t explain this subjective aspect of consciousness. Why is it that we picture a firetruck when we see a red water bottle? Why do we remember our parents as we stare at our kids? Why do we hear sounds from a Star Wars movie while reading a cookbook? Chalmers argues that “cognitive science” is capable of explaining all of the easy problems of consciousness, but it fails to explain this subjective, hard part of consciousness. Instead of something that can be explained by functions, the concept of experience remains unclear, even when every function is explained.</p><p id="7182cf37-c4da-4647-9fe5-432fc2b8b250" class="">
</p><p id="97173a41-9fe1-42c1-90bd-100bf2382483" class="">Various proposed theories of consciousness fail to answer the hard problems. For instance, Crick and Koch propose a “neurobiological theory of consciousness” which defines a basis for consciousness through 35-75 hertz neural oscillations (1990). Other researchers view consciousness as a sort of global engine. However, Chalmers points out that although information can be experienced through global accessibility (which was one of Baars’ central ideas), it fails to properly link global accessibility to experience. Other solutions, including many that simply deny the existence of consciousness, fail to provide an explanation for human experiences. Chalmers attempts to build the bridge between awareness (the physical structures associated with consciousness) and the abstract concept of experience - which is why his theory is (in my opinion) the most well-developed.</p><p id="0ba997c3-d1d6-4333-b81f-c73a3acf7c83" class="">
</p><h2 id="882eb818-0cdb-4e75-9c97-d93702416eb6" class=""><em>Practice Your Fundamentals</em></h2><p id="8ec60f7c-f281-4814-bd1b-f5bff161f447" class="">Chalmers begins his exploration into consciousness by positing that structure and dynamics are only relevant in a physical dimension, and fail to properly address consciousness. He argues that consciousness “is not a problem about physical structures and functions” and is instead something far more “fundamental.”  Chalmers compares consciousness to “mass, charge, and spacetime.” John Von Neumann and Wigner argued that wave-function collapse was evidence for cosmic consciousness. Chalmers’ conception of a fundamental theory of consciousness aligns with this idea (he believes in panpsychism).</p><p id="bc65f499-d5f6-499f-9306-aa7e77931efd" class="">
</p><p id="8915ca90-98fc-49f7-9473-a82c7872cb16" class="">After declaring consciousness as fundamental to the universe, Chalmers argues that consciousness does not interfere with physical laws, and thus it must act as an addition to an existing theory of the universe. (“A physical theory gives a theory of physical processes, and a psychophysical theory tells us how those processes give rise to an experience”). Despite being a rather grandiose generalization, Chalmers presents a rather compelling idea. When we treat consciousness as something more fundamental, and then compartmentalize it into a separate closed system than physical reality, it is far easier to actually generate a theory. In other words, a “Unified Theory of Consciousness” would be far easier to attain than a physical theory that attempts to describe experience.</p><p id="9755ff34-101f-4b0b-832a-3d9203011db6" class="">
</p><p id="507fb27f-9146-4282-9da3-7ea4a82985ba" class="">Chalmers&#x27; grand theory includes multiple laws (or psychophysical principles) that connect physical processes to experiences.</p><h2 id="53572f35-5cd3-4611-b608-58c544bd98e8" class=""><em>Structural Coherence</em></h2><p id="353003b4-bf06-4a74-b5db-8201612c11dd" class="">The first principle is the idea of structural coherence, which creates a fundamental link between awareness and experience. Chalmers describes that “whenever there is a conscious experience, there is some corresponding information in the cognitive system” that controls behavior (1990). Thus, there is a relationship between awareness (changes in physical structure) and the idea of consciousness. When considering this law, it becomes apparent that the processes that define awareness can provide at least a basis for a unified theory of consciousness. </p><p id="3c5737dd-0346-439f-9791-7ec2381bd367" class="">
</p><h2 id="48cea496-2954-4c87-b7c6-6382f41443a4" class=""><em>Organizational Invariance</em></h2><p id="d06333a3-00c9-4521-af4c-224fd9a2e9ee" class="">The second principle Chalmers proposes is the concept of organizational invariance. The principle states that “any two systems with the same fine-grained functional organization will have qualitatively identical experience” - relying on this principle, Chalmers argues that “if the casual patterns of neural organization were duplicated in silicon,” consciousness would arise. He then offers a rather intriguing thought experiment: when we focus, can we notice and observe changes in our conscious experience? If so, there is a more defined link between processing and experience which would further the case for organizational invariance.  Chalmers concludes that due to the strength of organizational invariance (as a theory), we can further narrow the concept of consciousness - “only physical properties directly relevant to the emergence of experience are organizational properties.”</p><p id="faf39e63-701f-4b16-ae23-34890e0a875b" class="">
</p><h2 id="e7c4c9e4-4b74-4674-8dc1-0319b0c809b5" class=""><em>The Dual-Aspect Theory of Information</em></h2><p id="936c0f45-152a-4f61-a217-b1326b800da6" class="">The last law in Chalmers’ unified theory is the dual-processing law of information. He begins his introduction of the theory by conceding that the previous two laws (structural coherence and organizational awareness) were rather “nonbasic.” He explains that these laws are simply helpful constraints rather than explanatory principles. His idea of a principle that actually can explain consciousness and also fits these helpful constraints surrounds the double-aspect principle of information. Drawing from Shannon (1948), he argues that there is a “direct isomorphism between certain physically embodied information space and certain phenomenal” information spaces. An information space is the sum of multiple embeddings made up of different information states. At its core, Chalmers defines an information space as a generalization of the ways “different elements in a space are similar or different, possibly in complex ways.”</p><p id="398804a0-b3e9-4e74-ae5e-34c598b33549" class="">
</p><p id="163d5562-df6a-429f-b8d6-f4c8f6aecb64" class="">If information has two aspects - a physical aspect (gathering information) and a phenomenal aspect - it would explain how certain physical functions can give rise to experiences.</p><p id="55a58002-4f99-4c5a-b390-fb0f691cfd86" class="">
</p><h2 id="f2c59543-79ec-48d5-9b8d-97301007a62e" class=""><em>Unified Theory</em></h2><p id="10caa1d7-a350-4a54-95cf-24b9f844a6a1" class="">Chalmers’ grand theory of consciousness is rooted in the ideas of structural coherence, organizational invariance, and dual-information processing. Using the constraints of the first two laws, Chalmers suggests that information has multiple states (a physical and phenomenal state). The first state gives rise to awareness, the second state gives rise to consciousness. The phenomenal state cannot be described by physical laws or properties, as it is derived from a basic fundamental constant of consciousness (this truly is the basis of panpsychism). A different functionalist perspective suggests that Chalmers’ phenomenal state can still be explained by processes (and an experimental aspect of information is unnecessary). It should be noted that Chalmers wasn’t exactly enthusiastic about this concept of abstract information. However, it fits his guidelines rather well and aligns with his idea of the mystical (nonphysical) nature of consciousness.</p><p id="2423da35-bc38-4872-b97a-c7257ddee449" class="">
</p><p id="bd0e4ad0-e834-4d0a-8483-4eee1dcf39ad" class="">I’ve read through a few theories and in my opinion, this one is the best so far. It provides enough physical grounding in neuroscience to keep it isolated from some solely metaphysical theories. However, it doesn’t try to explain such a complex topic with physical processes (which, as discussed earlier, is doomed to fail).</p><h1 id="444f7993-40e5-49a7-a6c4-6604bc052aa6" class="">Neural Networks</h1><p id="19cb4b30-fadd-4594-b95d-dbda42a1355f" class="">
</p><p id="3ec9517c-2fdf-4723-abfc-bf11ae2f761d" class="">Now we’ll move on to the math and structure behind neural networks.</p><p id="c291c798-66d3-4fce-9b65-4f2c2de3365c" class="">The vast majority of the information presented in the following sections is drawn from the 3B1B YouTube channel and resource page, which provides a nearly perfect exposition of the world of neural networks.</p><p id="e6bef196-8b52-41ce-bb71-1b0986fd3dce" class="">
</p><h2 id="4c28e1d5-3920-4d77-8a70-34ac94cd2939" class=""><em>Structure of a Neural Network</em></h2><p id="8cb68f6e-190c-4349-babe-aa92b9878331" class="">The first aspect of a neural network we’ll discuss is a neuron. The main job of neurons is to hold a number (the number is the “activation” of that neuron). When the number stored in a neuron is higher, you can imagine the neural as being “brighter.” The first layer of a neural network has a number of input neurons. These neurons typically relate to whatever data is being used for training. For instance, if you are training a neural network with a series of images, the input layer will be composed of neurons that relate to various pixels in an individual image. At the end of a neural network is an output layer that includes neurons directly associated with the predicted result. For instance, if the neural network is predicting TRUE or FALSE, the output layer will have two neurons. Each neuron will have a specific number (whichever activation is higher is its selection). In a perfect world, the final layer will include two neurons with activations of 1 and 0 (this is a completely confident TRUE). Finally, a neural network has a number of “hidden layers.” These layers are also also made up of neurons and contribute to calculations for the final layer. </p><p id="ddc3248f-5389-4cbc-a9cf-29bb4935ee15" class="">
</p><h2 id="e8d6bed2-cda9-4911-823a-8d81a82c87e2" class=""><em>Forward Propagation</em></h2><p id="8275ae94-4e44-4866-b488-fc0a33aff7fe" class="">Information passes through each layer and neuron through weights and biases. You can imagine the weights as the strings connecting neurons. A neuron in the second layer of a neural network is the sum of the weights and activations of the previous layers. (a2 = w1a1 + w2a2 + w3a3…). Additionally, each second layer neuron is added a specific bias (a number). Once everything is added up, it is compressed into a number between 0 and 1 using a sigmoid function. Continue this process across a neural network (starting with completely random weights and biases + numbers for the initial layer). You can create an activation number for every single neuron in layers 2 and beyond.</p><p id="d5b4fc05-ec1b-40d4-919c-834029fc74bf" class="">
</p><h2 id="a58a8576-44fb-440b-9815-289568c211fb" class=""><em>Backward Propagation</em></h2><p id="7828e772-3f5c-4680-845d-f7aeb651cb3e" class="">So we know how to get activations from weights and biases, but how do neural networks actually make correct predictions? This is the hard part. The first thing you need is a list of correct answers. For every training data input, there is an actual result and a predicted result. Once you have the correct answers, you can calculate the “cost” of your neural network. The cost is “how wrong” the network <a href="http://is.ONe">is</a>. One way to calculate the loss is to use squares of the differences between actual and predicted values). This cost function is the basis of backward propagation. The goal is to minimize this cost function by changing various weights and biases. The mathematics behind this overall minimization requires a prior understanding of multivariable calculus and linear algebra, so, with the intention of providing a general, clear explanation, I will only provide a generalization. To minimize the cost function, you find out how much a change in each weight and bias individually impacts the cost. Eventually, you compile a general gradient that includes all of the “nudges” needed for the weights and biases. This gradient will tell us how much we need to change each weight and bias to minimize the cost function. After repeating this process thousands of times, eventually, the cost lowers, and the neural network is able to make better predictions. </p><h2 id="c404f876-1932-4665-8ce4-3ee6b79a5074" class=""><em>Disclaimer</em></h2><p id="d73c8d54-12ca-46d7-bf4a-2d87e2b1af15" class="">This explanation is very simple. However, hopefully, it can give you a basic understanding of the algorithmic, almost simple way neural networks learn.</p><p id="39b0d7ea-c914-4c9b-ab0f-c70ded180bd5" class="">For more information, you can check out my github “Neural Network from Scratch” project that includes notes for backpropagation calculus: <a href="https://github.com/ATX24/November">https://github.com/ATX24/November</a></p><p id="413c8dfc-50e2-402b-a141-395ce9b2fab3" class="">Additionally, you can work through the 3B1B online lesson/video series for neural networks:</p><p id="a02ac5bd-61a6-4d6e-b2e7-762e2cf6b20d" class=""><a href="https://www.3blue1brown.com/lessons/gradient-descent">https://www.3blue1brown.com/lessons/</a></p><h1 id="5e6280a1-3dd2-430b-a253-6a5a154fbd98" class="">Artificial Consciousness</h1><h2 id="0a7a4eab-da9e-4eb2-b537-cef50eb1eb34" class=""><em>Basic Ideas</em></h2><p id="a6da5101-e23e-49f6-9570-1bda075adab7" class="">If AI does become conscious, the world will change instantly. Our modern conceptions of life, ethics, and machines will change dramatically. But how do you determine whether an AI is conscious? First, we’ll assume that future models will follow the basic ideas of a neural network. ChatGPT uses a transformer network, a powerful architecture for predictive analysis. However, the transformer model is inherently related to basic neural networks. Additionally, we’ll be making assumptions about consciousness based on Chalmers’ theory.</p><p id="9dfcc15d-1a73-4f87-b973-a1bb56ce2f94" class="">
</p><p id="a4829eb4-75b5-4ecf-9c9a-19feb7fc4da5" class="">So, how can Chalmers’ Unified Theory of Consciousness help us develop a better test of Artificial Intelligence? Well, first I think we need to redefine AI. ChatGPT and other language models are already pretty close to an agent (or combination of agents) capable of replicating human intelligence. Almost every piece of information ever shared on the internet has been consumed by these models, and as they continue to become more efficient, they may be able to better represent the data provided in training. However, these LLMs aren’t conscious. Researchers at Meta, Google, and OpenAI are building an intelligent system, not a conscious system. There is a fundamental difference between the two. Even if an AI can replicate human responses perfectly, it is incapable of thinking. Every single answer, no matter how complex or troubling, is the result of sophisticated calculus.</p><p id="f76d23f4-a719-4148-818b-a8462f2dee50" class="">
</p><h2 id="89d3a09f-4d7f-4cba-a2c2-2fa69317233b" class=""><em>Proposed Test</em></h2><p id="d74d486b-15ff-4f38-b50f-d219d6541a11" class="">The task of developing a test for Artificial Consciousness (AC) is an insurmountably difficult task due to our lack of understanding in the fields of neuroscience and our relatively abstract notion of consciousness. However, based on the strongest aspects of Chalmers’ theory, there are some basic principles that can test whether an algorithm has achieved AC.</p><p id="e9cf55b2-8db1-4ce5-80d0-2658ec2703fa" class="">
</p><p id="8cc17c80-ce71-4509-8b37-b7bcec65766f" class="">Test for AI Consciousness</p><ol type="1" id="b6fa6062-ab95-4a69-bf0c-3676ffc1ea96" class="numbered-list" start="1"><li>Structural coherence + Organizational Awareness: Does the system have the exact same structure and patterns as a human or animal brain?</li></ol><ol type="1" id="26080b82-a3ef-4a13-bab2-d2571d969ca2" class="numbered-list" start="2"><li>Is there evidence for a duality of processed information? As the AI becomes aware and processes information, does it also <em>experience the data? </em>In other words, does the system create a phenomenal information space where it creates complex, almost random correlations between collected data?</li></ol><p id="73633941-cbba-46bd-9c64-dbd46d2441a9" class="">
</p><p id="38b4ee74-0f3b-4b06-a86b-fecea40e8456" class="">If these two tests are passed, we can use the law of structural coherence, the law of organizational invariance, and the theory of dual information processing to classify the system as conscious.</p><p id="3aee94df-e4d7-4ca6-bbdb-355326f76878" class="">
</p><h2 id="37f48684-0c81-4d17-8318-5a9b45515d92" class=""><em>What could this look like in a neural network? Some general predictions about AI/Consciousness.</em></h2><p id="51ae0549-00e0-4bbd-b4dd-5c5813fdc6c1" class="">
</p><p id="f5fb8bda-eca4-49b0-8843-f3232b2257ba" class="">First, I don’t believe that we will see AC for at least another five hundred years (if not many more). The current systems that we have are not nearly sophisticated enough to capture the mystical idea of <em>experience. </em>However, if we were to create this advanced system in the future - that stems from the basic ideas of neural networks -  the system would be able to collect data, train itself, and also develop a simultaneous network of abstract correlations as it observes new information.</p><p id="b7e0be4f-28ed-4e33-9393-4f41b1b5458b" class="">
</p><p id="756bdf7d-398a-4772-9219-1111e2b5b619" class="">But all of this recent AI development is building toward an AGI or AC, right? This is the main problem with the recent AI buzz. In the next ten years, AI systems will become increasingly part of daily life. Transformer models will revolutionize web applications, daily processes, and even politics. However, AC is not a possibility in the next ten years. I predict that the systems developed by OpenAI and other companies will become increasingly complex and get closer to mimicking humans. However, these systems will eventually hit a limit, either on the hardware side or the algorithmic side. When this happens, this AI phase will die out. History books will remember this time as the first age of AI - an age defined by machine learning and efficiency.</p><p id="0aabee56-30d5-481c-8c63-ce47ca466a08" class="">
</p><p id="2c1a3450-ee6a-4d79-9a6a-19643b0e3c87" class="">The second age of AI is hundreds of years away and will bring about the effective end of mankind as the dominant species on Earth. In the way humans capture charge to make circuits, we will find a way to capture consciousness and breathe life into machines.</p><p id="370aa8e4-b270-4b8d-8789-7c6b4bc62617" class="">
</p><p id="c32f0de5-d534-44c9-8126-a1ff356c1b7b" class=""><em>But, again, this reality is generations away.</em></p><h1 id="05774333-beb5-44cc-8ab0-a8a15a2a503b" class="">Conclusion</h1><h2 id="c2764ff3-e3f4-4eb8-b6b6-83e776ad93f6" class=""><em>The Dangers of a Theory of Consciousness</em></h2><p id="1ccadd5b-1ef6-47c1-bd3f-14306b7f009d" class="">Yuval Noah Harari has a great lecture about the dangers associated with developing a theory of consciousness. If we were able to develop a fundamental idea of “how conscious” a system is, then we would be able to apply that idea in various dangerous ways. Imagine a dystopian future where some humans are labeled more conscious than others. Thus, researchers must be careful when trying to define consciousness.</p><h2 id="f443e13a-14eb-40b5-9ae3-095c944629e9" class=""><em>The Real Problem With AI</em></h2><p id="50f2d171-4266-47b4-a279-c4a0cee5ee03" class="">As mentioned earlier, the development of a destructive AC is generations away. However, current AI systems present various other pressing issues. The first obvious issue is deep fake technology. An environment where frighteningly convincing misinformation is mass-produced will quickly descend into chaos. </p><h2 id="2fa285e2-bab1-44ef-8441-ff66f2ae5302" class=""><em>Final Thoughts</em></h2><p id="59b069db-08aa-4736-81b8-5b481fdcaafe" class="">Consciousness is pretty abstract, Neural Networks are relatively simple, and Artificial Consciousness is an incredibly complex concept.</p><p id="2136390b-7844-48f2-bc40-a6f7d5e35220" class="">
</p><p id="cdc96880-6bf6-4994-9352-510cae282e2d" class="">However, based on our simple understanding of consciousness, we can develop a set of two laws to determine whether AC has been achieved in a system. Additionally, we can use these laws to construct a general understanding of what an AC system would be capable of.</p><p id="dce3e919-7b45-4aeb-90d4-e97523288e88" class="">
</p><p id="42aabfcf-7ba2-42d6-933d-7fccc8c1d493" class="">Finally, in our current age of AI, the real problem surrounding AI involves deep fake technology and misinformation rather than the Terminator.</p><p id="147d0f71-d940-4c1b-afff-35287a50c8fa" class="">
</p><h1 id="e951dec8-09ce-4202-808d-a150c1239b4e" class="">References</h1><p id="8cff6c97-c61e-4058-a96e-0200bad69283" class=""><a href="https://philpapers.org/rec/CHAFUT">https://philpapers.org/rec/CHAFUT</a></p><p id="b0efe9a4-83c3-4a6a-80d2-22c018de9240" class=""><a href="https://www.youtube.com/results?search_query=3b1b+neural+network">https://www.youtube.com/results?search_query=3b1b+neural+network</a> </p><p id="49475033-3af6-4386-8fe8-51589d204e33" class=""><a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6842945/#:~:text=The%20neural%20correlates%20of%20consciousness%20have%20been%20defined%20as%20the,Crick%20and%20Koch%2C%201990">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6842945/#:~:text=The neural correlates of consciousness have been defined as the,Crick and Koch%2C 1990</a>). </p></div></article><span class="sans" style="font-size:14px;padding-top:2em"></span></body></html>